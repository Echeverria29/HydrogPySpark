{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C_pyHKWy9IlK","outputId":"a9c37faa-6203-4294-a46e-cc76a513378c","executionInfo":{"status":"ok","timestamp":1726434218378,"user_tz":180,"elapsed":55178,"user":{"displayName":"Orlando Echeverria","userId":"05586657123196936485"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=881805267dbbc77a4e30f8b286bbbd755846f3517cd35813a8e8804b2a6a1e3c\n","  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.2\n"]}],"source":["pip install pyspark"]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, avg, max, min, when, row_number, unix_timestamp, from_unixtime, round,last, lower,regexp_replace,trim\n","from pyspark.sql.window import Window"],"metadata":{"id":"aDZRaZaY9ssq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Crear la sesión de Spark\n","spark = SparkSession.builder.appName(\"ETL_Local_Simulation\").getOrCreate()\n","\n","# Cargar el archivo CSV en un DataFrame de PySpark\n","df = spark.read.csv(\"/content/Generated_Dataset.csv\", header=True, inferSchema=True)\n","\n","# Verificar los nombres de columnas\n","df.printSchema()\n","\n","# Mostrar una muestra de datos para revisar el contenido\n","df.show(10, truncate=False)\n","\n","# Contar el número de filas en el DataFrame\n","num_filas = df.count()\n","\n","# Mostrar el resultado\n","print(f\"El DataFrame tiene {num_filas} filas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"4gVIkiZr_OjD","outputId":"20b52491-076d-4b32-de98-08c6a3fff8c3","executionInfo":{"status":"error","timestamp":1726434526294,"user_tz":180,"elapsed":301,"user":{"displayName":"Orlando Echeverria","userId":"05586657123196936485"}},"collapsed":true},"execution_count":null,"outputs":[{"output_type":"error","ename":"AnalysisException","evalue":"[PATH_NOT_FOUND] Path does not exist: file:/content/Generated_Dataset.csv.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-81aead59c724>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Cargar el archivo CSV en un DataFrame de PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/Generated_Dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Verificar los nombres de columnas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/Generated_Dataset.csv."]}]},{"cell_type":"markdown","source":["el correlativo que asignamos usando una ventana en PySpark no tiene que ver con agrupar los datos “de a 1” como mencionabas antes. Su propósito principal es ordenar las filas de acuerdo con la fecha y hora de lectura.\n","\n","Clarificación\n","No se Agrupa por 1:\n","\n","El correlativo no está agrupando las filas de una en una. Más bien, asigna un número consecutivo (un identificador único) a cada fila según el orden de fecha y hora de lectura.\n","Solo Ordena los Datos:\n","\n","Al usar la ventana para asignar el correlativo, simplemente estamos asegurándonos de que cada fila tenga un número único que sigue la secuencia temporal correcta. Esto es especialmente útil para cualquier cálculo que dependa del orden de las lecturas (como comparar una lectura con la anterior)."],"metadata":{"id":"-8_C_mJoBGyu"}},{"cell_type":"code","source":["# Crear una ventana para asignar un correlativo a cada lectura basado en la fecha y hora de lectura\n","window_spec = Window.orderBy(\"Fecha lectura\", \"Hora lectura\")\n","\n","# Asignar el correlativo de procesos\n","df = df.withColumn(\"Correlativo\", row_number().over(window_spec))"],"metadata":{"id":"YA5_aPBRA0gn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Determinar la dirección de cada lectura ('Bajada' o 'Subida')\n","df = df.withColumn(\n","    \"Dirección\", when(col(\"Variable\").contains(\"bajada\"), \"Bajada\").otherwise(\"Subida\")\n",")\n"],"metadata":{"id":"_as9TiqzA1-r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convertir la columna de fecha y hora a timestamp\n","df = df.withColumn(\"Hora lectura\", unix_timestamp(\"Hora lectura\", \"yyyy-MM-dd HH:mm:ss\"))"],"metadata":{"id":"Sr-Hgp6eA37h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Agrupar lecturas en intervalos de tiempo (por ejemplo, cada minuto)\n","df_grouped = df.groupBy(\n","    from_unixtime((col(\"Hora lectura\") / 60).cast(\"long\") * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Desde\"),\n","    from_unixtime(((col(\"Hora lectura\") / 60).cast(\"long\") + 1) * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Hasta\"),\n","    \"Dirección\"\n",").agg(\n","    avg(\"valor\").alias(\"V Promedio\"),\n","    max(\"valor\").alias(\"V Max\"),\n","    min(\"valor\").alias(\"V Min\"),\n","    avg(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Promedio\"),\n","    max(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Max\"),\n","    min(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Min\")\n",")\n"],"metadata":{"id":"-Elb6GqlBSL6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Redondear los valores a 2 decimales usando la función 'round' de PySpark\n","df_grouped = df_grouped.withColumn(\"V Promedio\", round(col(\"V Promedio\"), 2)) \\\n","                       .withColumn(\"V Max\", round(col(\"V Max\"), 2)) \\\n","                       .withColumn(\"V Min\", round(col(\"V Min\"), 2)) \\\n","                       .withColumn(\"P Promedio\", round(col(\"P Promedio\"), 2)) \\\n","                       .withColumn(\"P Max\", round(col(\"P Max\"), 2)) \\\n","                       .withColumn(\"P Min\", round(col(\"P Min\"), 2))"],"metadata":{"id":"VljJhwmABT-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Mostrar el resultado final\n","df_grouped.show(10, truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDlCX7LqBWIU","outputId":"c3b49115-e9c4-4145-aeb1-02989f30a876","executionInfo":{"status":"ok","timestamp":1726165350073,"user_tz":180,"elapsed":3715,"user":{"displayName":"Nicolas Almarza","userId":"15016183795733313193"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","|Desde              |Hasta              |Dirección|V Promedio|V Max  |V Min|P Promedio|P Max  |P Min  |\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","|2024-09-12 00:19:00|2024-09-12 00:20:00|Bajada   |3.22      |3.22   |3.22 |NULL      |NULL   |NULL   |\n","|2024-09-12 00:47:00|2024-09-12 00:48:00|Subida   |418.28    |833.63 |2.92 |833.63    |833.63 |833.63 |\n","|2024-09-12 04:48:00|2024-09-12 04:49:00|Subida   |431.88    |860.77 |2.98 |860.77    |860.77 |860.77 |\n","|2024-09-12 05:10:00|2024-09-12 05:11:00|Bajada   |4.77      |4.77   |4.77 |NULL      |NULL   |NULL   |\n","|2024-09-12 06:50:00|2024-09-12 06:51:00|Subida   |529.62    |1056.23|3.0  |1056.23   |1056.23|1056.23|\n","|2024-09-12 07:47:00|2024-09-12 07:48:00|Bajada   |2.76      |2.76   |2.76 |NULL      |NULL   |NULL   |\n","|2024-09-12 07:53:00|2024-09-12 07:54:00|Subida   |599.55    |1196.64|2.46 |1196.64   |1196.64|1196.64|\n","|2024-09-12 09:09:00|2024-09-12 09:10:00|Bajada   |4.03      |4.03   |4.03 |NULL      |NULL   |NULL   |\n","|2024-09-12 09:30:00|2024-09-12 09:31:00|Bajada   |2.94      |2.94   |2.94 |NULL      |NULL   |NULL   |\n","|2024-09-12 12:18:00|2024-09-12 12:19:00|Bajada   |3.97      |3.97   |3.97 |NULL      |NULL   |NULL   |\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","only showing top 10 rows\n","\n"]}]},{"cell_type":"code","source":["# Contar el número de filas en el DataFrame\n","num_filas = df_grouped.count()\n","\n","# Mostrar el resultado\n","print(f\"El DataFrame tiene {num_filas} filas.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiqx7VVldZev","executionInfo":{"status":"ok","timestamp":1726165377489,"user_tz":180,"elapsed":1293,"user":{"displayName":"Nicolas Almarza","userId":"15016183795733313193"}},"outputId":"5f8c9bf0-af41-47c2-ffc9-126c430edf96"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El DataFrame tiene 2000 filas.\n"]}]},{"cell_type":"code","source":["# Contar registros de presión para la dirección \"Bajada\"\n","df.filter((col(\"Variable\") == \"Presion\") & (col(\"Dirección\") == \"Bajada\")).count()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ce1Zw98UI_Iq","outputId":"d3e40715-6466-4d7a-dd43-d31d7f042abf","executionInfo":{"status":"ok","timestamp":1726165354997,"user_tz":180,"elapsed":832,"user":{"displayName":"Nicolas Almarza","userId":"15016183795733313193"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, avg, max, min, when, row_number, lit, round, unix_timestamp, from_unixtime\n","from pyspark.sql.window import Window\n","\n","# Crear la sesión de Spark\n","spark = SparkSession.builder.appName(\"ETL_Local_Simulation\").getOrCreate()\n","\n","# Cargar el archivo CSV en un DataFrame de PySpark\n","df = spark.read.csv(\"/content/Generated_Datasetnico.csv\", header=True, inferSchema=True)\n","# Contar el número de filas en el DataFrame\n","num_filas = df.count()\n","print(f\"El DataFrame tiene {num_filas} filas después del procesamiento.\")\n","\n","# Tratamiento de valores nulos\n","# Calcular los promedios de las columnas relevantes\n","valor_promedio = df.select(avg(\"valor\")).first()[0]\n","\n","# Reemplazar valores nulos en la columna \"valor\" por su promedio calculado\n","df = df.fillna({'valor': valor_promedio})\n","\n","# Reemplazar valores nulos en columnas de texto por 'Desconocido'\n","df = df.fillna({'Variable': 'Desconocido', 'Unidad': 'Desconocido'})\n","\n","# Eliminar filas con valores nulos en columnas clave de fechas\n","df = df.na.drop(subset=['Fecha lectura', 'Hora lectura'])\n","\n","# Crear una ventana para asignar un correlativo a cada lectura basado en la fecha y hora de lectura\n","window_spec = Window.orderBy(\"Fecha lectura\", \"Hora lectura\")\n","\n","# Asignar el correlativo de procesos\n","df = df.withColumn(\"Correlativo\", row_number().over(window_spec))\n","\n","# Determinar la dirección de cada lectura ('Bajada' o 'Subida')\n","df = df.withColumn(\n","    \"Dirección\", when(col(\"Variable\").contains(\"bajada\"), \"Bajada\").otherwise(\"Subida\")\n",")\n","\n","# Convertir la columna de fecha y hora a timestamp\n","df = df.withColumn(\"Hora lectura\", unix_timestamp(\"Hora lectura\", \"yyyy-MM-dd HH:mm:ss\"))\n","\n","# Agrupar lecturas en intervalos de tiempo (por ejemplo, cada minuto)\n","df_grouped = df.groupBy(\n","    from_unixtime((col(\"Hora lectura\") / 60).cast(\"long\") * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Desde\"),\n","    from_unixtime(((col(\"Hora lectura\") / 60).cast(\"long\") + 1) * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Hasta\"),\n","    \"Dirección\"\n",").agg(\n","    avg(\"valor\").alias(\"V Promedio\"),\n","    max(\"valor\").alias(\"V Max\"),\n","    min(\"valor\").alias(\"V Min\"),\n","    avg(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Promedio\"),\n","    max(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Max\"),\n","    min(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Min\")\n",")\n","\n","# Calcular los promedios de las columnas de presión globalmente\n","p_promedio_global = df_grouped.select(avg(\"P Promedio\")).first()[0]\n","p_max_global = df_grouped.select(avg(\"P Max\")).first()[0]\n","p_min_global = df_grouped.select(avg(\"P Min\")).first()[0]\n","\n","# Reemplazar los valores nulos con el promedio global correspondiente\n","df_grouped = df_grouped.withColumn(\"P Promedio\", when(col(\"P Promedio\").isNull(), p_promedio_global).otherwise(col(\"P Promedio\"))) \\\n","                       .withColumn(\"P Max\", when(col(\"P Max\").isNull(), p_max_global).otherwise(col(\"P Max\"))) \\\n","                       .withColumn(\"P Min\", when(col(\"P Min\").isNull(), p_min_global).otherwise(col(\"P Min\")))\n","\n","# Redondear los valores a 2 decimales usando la función 'round' de PySpark\n","df_grouped = df_grouped.withColumn(\"V Promedio\", round(col(\"V Promedio\"), 2)) \\\n","                       .withColumn(\"V Max\", round(col(\"V Max\"), 2)) \\\n","                       .withColumn(\"V Min\", round(col(\"V Min\"), 2)) \\\n","                       .withColumn(\"P Promedio\", round(col(\"P Promedio\"), 2)) \\\n","                       .withColumn(\"P Max\", round(col(\"P Max\"), 2)) \\\n","                       .withColumn(\"P Min\", round(col(\"P Min\"), 2))\n","\n","# Mostrar el resultado final\n","df_grouped.show(10, truncate=False)\n","# Contar el número de filas en el DataFrame\n","num_filas = df_grouped.count()\n","print(f\"El DataFrame tiene {num_filas} filas después del procesamiento.\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FYcC8G5pWmH","executionInfo":{"status":"ok","timestamp":1726241183642,"user_tz":180,"elapsed":5547,"user":{"displayName":"Orlando Echeverria","userId":"05586657123196936485"}},"outputId":"ac1a4b25-58ba-4e4c-898b-b508535f3ca5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["El DataFrame tiene 3000 filas después del procesamiento.\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","|Desde              |Hasta              |Dirección|V Promedio|V Max  |V Min|P Promedio|P Max  |P Min  |\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","|2024-09-13 00:33:00|2024-09-13 00:34:00|Subida   |612.1     |1220.3 |3.89 |1220.3    |1220.3 |1220.3 |\n","|2024-09-13 01:10:00|2024-09-13 01:11:00|Bajada   |4.21      |4.21   |4.21 |1158.55   |1158.55|1158.55|\n","|2024-09-13 02:37:00|2024-09-13 02:38:00|Bajada   |4.76      |4.76   |4.76 |1158.55   |1158.55|1158.55|\n","|2024-09-13 02:50:00|2024-09-13 02:51:00|Bajada   |2.33      |2.33   |2.33 |1158.55   |1158.55|1158.55|\n","|2024-09-13 03:52:00|2024-09-13 03:53:00|Subida   |489.98    |977.16 |2.81 |977.16    |977.16 |977.16 |\n","|2024-09-13 03:56:00|2024-09-13 03:57:00|Subida   |541.69    |1080.11|3.27 |1080.11   |1080.11|1080.11|\n","|2024-09-13 05:15:00|2024-09-13 05:16:00|Subida   |428.65    |854.89 |2.4  |854.89    |854.89 |854.89 |\n","|2024-09-13 08:40:00|2024-09-13 08:41:00|Subida   |469.17    |936.14 |2.2  |936.14    |936.14 |936.14 |\n","|2024-09-13 09:33:00|2024-09-13 09:34:00|Bajada   |1.18      |1.18   |1.18 |1158.55   |1158.55|1158.55|\n","|2024-09-13 12:13:00|2024-09-13 12:14:00|Bajada   |2.91      |2.91   |2.91 |1158.55   |1158.55|1158.55|\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+-------+\n","only showing top 10 rows\n","\n","El DataFrame tiene 2000 filas después del procesamiento.\n"]}]},{"cell_type":"code","source":["\n","# Guardar la tabla transformada en un archivo CSV, permitiendo sobrescribir si el archivo ya existe\n","df_grouped.write.mode(\"overwrite\").csv(\"salida_ETL_formato_final.csv\", header=True)\n"],"metadata":{"id":"tojsXgPu-6da"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[":SOLUCION NO TOMA PRESION DE BAJADA 3  13-09-2024   (LA EJECUCION DEMORA 5 SEGUNDOS SI SE EJECUTA COMPLETA NO POR PARTES)\n","\n"],"metadata":{"id":"xarWADDt2oXL"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, avg, max, min, when, row_number, unix_timestamp, from_unixtime, round,last, lower,regexp_replace,trim\n","from pyspark.sql.window import Window\n","\n","# Crear la sesión de Spark\n","spark = SparkSession.builder.appName(\"ETL_Local_Simulation\").getOrCreate()\n","\n","# Cargar el archivo CSV en un DataFrame de PySpark\n","df = spark.read.csv(\"Expanded_Dataset_with_Corrected_Times.csv\", header=True, inferSchema=True)\n","# Verificar los nombres de columnas\n","df.printSchema()\n","\n","# Mostrar una muestra de datos para revisar el contenido\n","df.show(50, truncate=False)\n","# Contar el número de filas en el DataFrame\n","num_filas = df.count()\n","print(f\"El DataFrame tiene {num_filas} filas al cargar.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSRu4Iw_AiZf","executionInfo":{"status":"ok","timestamp":1726437225077,"user_tz":180,"elapsed":2177,"user":{"displayName":"Orlando Echeverria","userId":"05586657123196936485"}},"outputId":"fde35dd4-72f3-4676-de4c-319512e0c82e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["root\n"," |-- Variable: string (nullable = true)\n"," |-- Tipo Dato: string (nullable = true)\n"," |-- Unidad: string (nullable = true)\n"," |-- Fecha lectura: date (nullable = true)\n"," |-- Hora lectura: timestamp (nullable = true)\n"," |-- valor: double (nullable = true)\n","\n","+----------------+---------+------+-------------+-------------------+-------+\n","|Variable        |Tipo Dato|Unidad|Fecha lectura|Hora lectura       |valor  |\n","+----------------+---------+------+-------------+-------------------+-------+\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:00:00|3.54   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:00:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:00:00|3.31   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:00:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:01:00|0.56   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:01:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:01:00|1146.32|\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:01:00|3.54   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:02:00|3.54   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:02:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:02:00|3.31   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:02:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:03:00|0.56   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:03:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:03:00|1146.32|\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:03:00|3.54   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:04:00|0.56   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:04:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:04:00|1146.32|\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:04:00|1146.32|\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:05:00|3.54   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:05:00|3.54   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:05:00|3.54   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:05:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:06:00|1146.32|\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:06:00|3.54   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:06:00|3.54   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:06:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:07:00|3.31   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:07:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:07:00|0.56   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:07:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:08:00|1146.32|\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:08:00|3.54   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:08:00|3.54   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:08:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:09:00|3.31   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:09:00|0.56   |\n","|Velocidad bajada|Num      |m/s   |2024-09-15   |2024-09-15 00:09:00|3.54   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:09:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:10:00|3.31   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:10:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:10:00|0.56   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:10:00|1146.32|\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:11:00|3.31   |\n","|Presion         |Num      |PSI   |2024-09-15   |2024-09-15 00:11:00|1146.32|\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:11:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:11:00|0.56   |\n","|Velocidad subida|Num      |m/s   |2024-09-15   |2024-09-15 00:12:00|0.56   |\n","|Recorrido       |Num      |m     |2024-09-15   |2024-09-15 00:12:00|3.31   |\n","+----------------+---------+------+-------------+-------------------+-------+\n","only showing top 50 rows\n","\n","El DataFrame tiene 96000 filas al cargar.\n"]}]},{"cell_type":"code","source":["  #Crear una ventana para asignar un correlativo a cada lectura basado en la fecha y hora de lectura\n","window_spec = Window.orderBy(\"Fecha lectura\", \"Hora lectura\")\n","\n","# Asignar el correlativo de procesos\n","df = df.withColumn(\"Correlativo\", row_number().over(window_spec))\n","\n"],"metadata":{"id":"zOxbsf31e-Wy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","Sensibilidad del Comportamiento de PySpark\n","Internalización de Datos en PySpark: Parece que PySpark está manejando internamente los datos de manera peculiar. Hay algunas razones posibles para esto:\n","Optimización Interna de PySpark: PySpark podría estar realizando una optimización en segundo plano, donde la búsqueda de la cadena \"Bajada\" (con mayúscula) es más directa debido a cómo está almacenando los datos internamente. Esto puede evitar el procesamiento adicional que ocurre cuando se usan otras manipulaciones de cadenas.\n","Uso de un Código Interno de Python o JVM: La función contains podría estar utilizando métodos internos que interpretan \"Bajada\" de una manera específica cuando se llama directamente con la mayúscula, tal vez debido a cómo se maneja la sensibilidad de las cadenas en Java, que PySpark utiliza bajo el capó.\n","Solución Final\n","Dado que el comportamiento peculiar parece estar funcionando correctamente cuando se usa contains(\"Bajada\") con \"B\" mayúscula, mi recomendación sería seguir utilizando este enfoque. Aunque no es intuitivo, a veces las peculiaridades de una biblioteca o framework requieren soluciones pragmáticas que aprovechen esas mismas peculiaridades.\n","\n"],"metadata":{"id":"r92FT0rzfBkd"}},{"cell_type":"code","source":["##############################################################\n","############ ACA ESTABA EL PROBLEMA #######################\n","########### EN EL PRIMER CONSTAINS ESTABA CON bajada en minuscula , pero si lo damos vuelta y colocamos Bajada}\n","# por alguna razon si lo toma aunque realmente en el data frame si este en minuscula\n","# Determinar la dirección de cada lectura ('Bajada' o 'Subida')\n","df = df.withColumn(\n","    \"Dirección\", when(col(\"Variable\").contains(\"Bajada\"), \"bajada\").otherwise(\"subida\")\n",")\n","##############################################################\n","##############################################################\n","##############################################################\n"],"metadata":{"id":"zMhIxAx5A39U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Convertir la columna de fecha y hora a timestamp\n","df = df.withColumn(\"Hora lectura\", unix_timestamp(\"Hora lectura\", \"yyyy-MM-dd HH:mm:ss\"))\n","\n","# Agrupar lecturas en intervalos de tiempo (por ejemplo, cada minuto)\n","\n","##########################\n","#colocar como variable OJO LOS MINUTOS\n","##########################\n","\n","df_grouped = df.groupBy(\n","    from_unixtime((col(\"Hora lectura\") / 60).cast(\"long\") * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Desde\"),\n","    from_unixtime(((col(\"Hora lectura\") / 60).cast(\"long\") + 1) * 60, \"yyyy-MM-dd HH:mm:ss\").alias(\"Hasta\"),\n","    \"Dirección\"\n",").agg(\n","    avg(\"valor\").alias(\"V Promedio\"),\n","    max(\"valor\").alias(\"V Max\"),\n","    min(\"valor\").alias(\"V Min\"),\n","    avg(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Promedio\"),\n","    max(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Max\"),\n","    min(when(col(\"Variable\") == \"Presion\", col(\"valor\"))).alias(\"P Min\")\n",")\n","\n","##########################\n","#colocar como variable OJO\n","##########################\n","\n","# Redondear los valores a 2 decimales usando la función 'round' de PySpark\n","df_grouped = df_grouped.withColumn(\"V Promedio\", round(col(\"V Promedio\"), 2)) \\\n","                       .withColumn(\"V Max\", round(col(\"V Max\"), 2)) \\\n","                       .withColumn(\"V Min\", round(col(\"V Min\"), 2)) \\\n","                       .withColumn(\"P Promedio\", round(col(\"P Promedio\"), 2)) \\\n","                       .withColumn(\"P Max\", round(col(\"P Max\"), 2)) \\\n","                       .withColumn(\"P Min\", round(col(\"P Min\"), 2))"],"metadata":{"id":"r9Hj8t2rfAcJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Cálculo Matemático de Agrupación\n","Para asegurarnos de que el cálculo es correcto, consideremos lo siguiente:\n","\n","Promedio de Registros por Minuto:\n","\n","Si comienzas con 4000 registros distribuidos en 60 minutos, el promedio de registros por minuto sería:\n","Promedio de registros por minuto\n","=\n","4000\n"," registros\n","60\n"," minutos\n","≈\n","66.67\n"," registros/minuto\n","Promedio de registros por minuto=\n","60 minutos\n","4000 registros\n","​\n"," ≈66.67 registros/minuto\n","Registros Finales Agrupados:\n","\n","Después de la agrupación, tener 264 registros sugiere que hay menos minutos con lecturas o múltiples lecturas se combinan en menos intervalos de tiempo de 1 minuto.\n","Finalmente, los 1000 registros podrían ser el resultado de múltiples agregaciones por diferentes combinaciones de Desde, Hasta y Dirección.\n","Conclusión\n","Parece que tus matemáticas son correctas: empezaste con 4000 registros y, al agrupar por minuto, llegaste a 264 registros. Luego, si esos registros tienen distintas direcciones o tipos de variables que agregaste, es razonable que termines con 1000 registros al final."],"metadata":{"id":"AfVUe7fMjEYz"}},{"cell_type":"code","source":["\n","\n","# Mostrar el resultado final\n","df_grouped.show(10, truncate=False)\n","\n","# Contar el número de filas en el DataFrame\n","num_filas_final = df_grouped.count()\n","print(f\"El DataFrame tiene {num_filas_final} filas después del procesamiento.\")\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dz6d8kXU9rrU","executionInfo":{"status":"ok","timestamp":1726437255114,"user_tz":180,"elapsed":3646,"user":{"displayName":"Orlando Echeverria","userId":"05586657123196936485"}},"outputId":"e47a1e41-fab0-44cc-f9fa-b75c42ffd70d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------------+-------------------+---------+----------+-------+-----+----------+-------+------+\n","|Desde              |Hasta              |Dirección|V Promedio|V Max  |V Min|P Promedio|P Max  |P Min |\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+------+\n","|2024-09-15 00:29:00|2024-09-15 00:30:00|subida   |391.48    |1456.85|0.73 |1203.95   |1456.85|864.01|\n","|2024-09-15 00:59:00|2024-09-15 01:00:00|subida   |285.74    |1491.12|0.96 |1204.83   |1491.12|835.12|\n","|2024-09-15 03:54:00|2024-09-15 03:55:00|subida   |283.69    |1475.63|1.15 |1125.5    |1475.63|850.46|\n","|2024-09-15 07:39:00|2024-09-15 07:40:00|subida   |307.87    |1494.43|0.85 |1154.41   |1494.43|865.88|\n","|2024-09-15 12:10:00|2024-09-15 12:11:00|subida   |165.79    |1440.07|1.02 |1110.39   |1440.07|883.61|\n","|2024-09-15 19:20:00|2024-09-15 19:21:00|subida   |205.11    |1492.34|0.54 |1081.31   |1492.34|820.08|\n","|2024-09-15 22:30:00|2024-09-15 22:31:00|subida   |296.03    |1400.43|0.78 |1106.51   |1400.43|836.82|\n","|2024-09-15 00:14:00|2024-09-15 00:15:00|subida   |285.49    |1473.67|0.54 |1204.37   |1473.67|849.76|\n","|2024-09-15 00:15:00|2024-09-15 00:16:00|subida   |389.93    |1483.86|0.54 |1256.49   |1483.86|849.76|\n","|2024-09-15 03:07:00|2024-09-15 03:08:00|subida   |317.86    |1494.48|0.75 |1193.0    |1494.48|964.8 |\n","+-------------------+-------------------+---------+----------+-------+-----+----------+-------+------+\n","only showing top 10 rows\n","\n","El DataFrame tiene 1440 filas después del procesamiento.\n"]}]},{"cell_type":"code","source":["\n","# Guardar la tabla transformada en un archivo CSV, permitiendo sobrescribir si el archivo ya existe\n","df_grouped.write.mode(\"overwrite\").csv(\"salida_ETL_formato_final.csv\", header=True)\n"],"metadata":{"id":"mvSOnavg-izQ"},"execution_count":null,"outputs":[]}]}